{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d477d38b-ce9a-4c25-9d35-cab506d7df6b",
   "metadata": {},
   "source": [
    "**Hard Braking Detection from Drone Videos**\n",
    "\n",
    "This project detects hard braking events at high-speed signalized intersections using drone footage. The system employs a customized YOLO model for vehicle detection, ByteTrack for tracking, and a homography transformation to convert pixel-based movements into real-world speed estimates.\n",
    "\n",
    "How It Works:\n",
    "\n",
    "1. **Vehicle Detection & Tracking**:\n",
    "\n",
    "   - YOLO identifies vehicles, while ByteTrack assigns unique IDs for consistent tracking across frames.\n",
    "\n",
    "3. **Speed Estimation**:\n",
    "\n",
    "   - A perspective transform converts pixel distances to real-world speeds (mph) using homography scaling.\n",
    "\n",
    "5. **Turning Movement Associations**:\n",
    "\n",
    "   - Vehicles are associated with their respective turning movements by detecting when their buffered bounding boxes cross predefined lane lines.\n",
    "\n",
    "7. **Hard Braking Identification**:\n",
    "\n",
    "\n",
    "   - A 20-frame Sliding Window Technique is used to track speed drops.\n",
    "\n",
    "   - A 70%+ speed reduction within the window, from speeds over 15 mph, flags potential hard braking.\n",
    "\n",
    "   - The next 100 frames are analyzed to confirm no sudden speed recovery, eliminating false positives.\n",
    "  \n",
    "   - The last 20 frames before braking ensure the vehicle was previously traveling at a stable speed.\n",
    "\n",
    "8. **Output**:\n",
    "\n",
    "\n",
    "   - A processed video visualizing detected vehicles, speeds, and lane tags.\n",
    "     \n",
    "   - An Excel file listing hard braking incidents with frame ranges, initial/final speeds, and percentage drop analysis.\n",
    "\n",
    "This methodology enables data-driven intersection safety analysis, helping identify areas prone to abrupt braking events and potential collision risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e552fe1e-ec71-4909-bb4c-3a7c7c76cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q opencv-python numpy pandas tqdm ultralytics supervision xlsxwriter\n",
    "\n",
    "# Import essential libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import supervision as sv\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from supervision.assets import VideoAssets, download_assets\n",
    "from collections import defaultdict, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357b5550-5142-4697-bca5-5e4b39798549",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# File Paths: Define paths for input video, output video, and trajectory data storage\n",
    "EXCEL_OUTPUT_PATH = \"/Users/gideonowusu/Downloads/PhD 2028/Projects/UAV-TrafficVision/Trajectory Data.xlsx\"\n",
    "SOURCE_VIDEO_PATH = '/Users/gideonowusu/Downloads/PhD 2028/Projects/UAV-TrafficVision/Vehicle Speed Estimation from Drone Footage/High-Speed Intersection Video Sample.mp4'\n",
    "TARGET_VIDEO_PATH = '/Users/gideonowusu/Downloads/PhD 2028/Projects/UAV-TrafficVision/Vehicle Speed Estimation from Drone Footage/High-Speed Intersection Video Sample Output.mp4'\n",
    "FINAL_HARDBRAKING_DATA = \"/content/gdrive/MyDrive/Hard Braking/Trial Hard Braking Data.xlsx\"\n",
    "# Real-World Measurements: Convert pixel values to meters using a known reference\n",
    "REAL_WORLD_WIDTH_M = 18  \n",
    "REAL_WORLD_HEIGHT_M = 120  \n",
    "PIXEL_WIDTH_PX = 25        \n",
    "PIXEL_HEIGHT_PX = 250      \n",
    "\n",
    "# Compute the scaling ratios to convert pixel measurements to meters\n",
    "width_ratio = REAL_WORLD_WIDTH_M / PIXEL_WIDTH_PX\n",
    "height_ratio = REAL_WORLD_HEIGHT_M / PIXEL_HEIGHT_PX\n",
    "\n",
    "# Perspective Transformation: Define the polygon for transformation (original → top-down view)\n",
    "initial_polygon = np.array([[270, 350], [460, 320], [1900, 660], [1720, 850]])  \n",
    "final_transformed = np.array([[0, 0], [24, 0], [24, 249], [0, 249]])  \n",
    "\n",
    "# Function to create the smallest buffered bounding box\n",
    "def create_smallest_buffered_box(bbox, num_steps):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    center_x = (x1 + x2) / 2\n",
    "    center_y = (y1 + y2) / 2\n",
    "    step_x = (center_x - x1) / num_steps\n",
    "    step_y = (center_y - y1) / num_steps\n",
    "\n",
    "    buffered_x1 = x1 + step_x\n",
    "    buffered_y1 = y1 + step_y\n",
    "    buffered_x2 = x2 - step_x\n",
    "    buffered_y2 = y2 - step_y\n",
    "\n",
    "    return (buffered_x1, buffered_y1, buffered_x2, buffered_y2)\n",
    "\n",
    "# Function to check if line segments (p1, p2) and (q1, q2) intersect\n",
    "def lines_intersect(p1, p2, q1, q2):\n",
    "    def ccw(A, B, C):\n",
    "        return (C[1] - A[1]) * (B[0] - A[0]) > (B[1] - A[1]) * (C[0] - A[0])\n",
    "\n",
    "    return ccw(p1, q1, q2) != ccw(p2, q1, q2) and ccw(p1, p2, q1) != ccw(p1, p2, q2)\n",
    "\n",
    "# Function to check if a line intersects a bounding box\n",
    "def line_intersects_bbox(p1, p2, bbox):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    rect = [(x1, y1), (x2, y1), (x2, y2), (x1, y2)]\n",
    "\n",
    "    for i in range(4):\n",
    "        if lines_intersect(p1, p2, rect[i], rect[(i + 1) % 4]):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Compute the transformation matrix to correct the perspective\n",
    "def get_perspective_transform(source, target):\n",
    "    \"\"\"Computes the transformation matrix from source to target perspective.\"\"\"\n",
    "    return cv2.getPerspectiveTransform(source.astype(np.float32), target.astype(np.float32))\n",
    "\n",
    "# Function to apply the transformation to points\n",
    "def warp_points(points, transform_matrix):\n",
    "    \"\"\"Applies a perspective warp transformation to a set of 2D points.\"\"\"\n",
    "    if not np.any(points):  \n",
    "        return points\n",
    "    float_points = np.expand_dims(points.astype(np.float32), axis=1)  \n",
    "    warped = cv2.perspectiveTransform(float_points, transform_matrix)  \n",
    "    return np.squeeze(warped, axis=1)  \n",
    "\n",
    "# Load customized YOLO Model for Drone Vehicle Detection (Use model of your choice)\n",
    "model = YOLO(\"/Users/gideonowusu/Downloads/PhD 2028/Projects/UAV-TrafficVision/Neuves.pt\")  \n",
    "\n",
    "# Get video metadata (frame rate, resolution, etc.)\n",
    "video_info = sv.VideoInfo.from_video_path(video_path=SOURCE_VIDEO_PATH)\n",
    "\n",
    "# Initialize ByteTrack for Object Tracking\n",
    "byte_track = sv.ByteTrack(\n",
    "    frame_rate=video_info.fps,  \n",
    "    track_activation_threshold=0.25,  \n",
    "    minimum_matching_threshold=0.8,  \n",
    "    lost_track_buffer=30  \n",
    ")\n",
    "\n",
    "# Annotators for Drawing on Video Frames\n",
    "bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=2)  \n",
    "label_annotator = sv.LabelAnnotator(text_scale=0.5, text_thickness=1, text_position=sv.Position.TOP_CENTER)  \n",
    "\n",
    "# Define a Region of Interest (ROI) for Tracking\n",
    "polygon_zone = sv.PolygonZone(polygon=initial_polygon)  \n",
    "\n",
    "# Dictionary to store Needed Info\n",
    "coordinates = defaultdict(lambda: deque(maxlen=video_info.fps))  \n",
    "trajectory_data = []  \n",
    "object_tags = {}\n",
    "vehicles_intersected_lines = set()\n",
    "\n",
    "# Compute the transformation matrix once for efficiency\n",
    "perspective_matrix = get_perspective_transform(initial_polygon, final_transformed)\n",
    "\n",
    "# Frame Processing Function (Called for Each Video Frame)\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    \"\"\"Processes each video frame: detects vehicles, tracks them, and estimates speed.\"\"\"\n",
    "\n",
    "    global trajectory_data  \n",
    "\n",
    "    # Run YOLO detection on the frame\n",
    "    results = model(frame, verbose=False)[0]\n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "\n",
    "    # Separate detected utility poles (class_id = 1) from vehicles \n",
    "    pole_detections = detections[detections.class_id == 1]\n",
    "    non_pole_detections = detections[detections.class_id != 1]\n",
    "\n",
    "    # Only track vehicles inside the defined polygon (ignoring background detections)\n",
    "    non_pole_detections = non_pole_detections[polygon_zone.trigger(non_pole_detections)]\n",
    "    final_detections = byte_track.update_with_detections(detections=non_pole_detections)\n",
    "\n",
    "    # Draw Turning Movement Lane Lines\n",
    "    for line in initial_lines:\n",
    "            frame = cv2.line(frame, line[\"start\"], line[\"end\"], (255, 0, 0), 2)\n",
    "\n",
    "    # Compute vehicle positions\n",
    "    for tracker_id, bbox in zip(final_detections.tracker_id, final_detections.xyxy):\n",
    "\n",
    "        buffered_bbox = create_smallest_buffered_box(bbox, num_steps=2)\n",
    "\n",
    "        if tracker_id not in object_tags and tracker_id is not None:\n",
    "            for line in initial_lines:\n",
    "                if line_intersects_bbox(line[\"start\"], line[\"end\"], buffered_bbox):\n",
    "                            # Associate the vehicle with the first line it intersects\n",
    "                    object_tags[tracker_id] = line[\"tag\"]\n",
    "                    vehicles_intersected_lines.add(tracker_id)\n",
    "                    break\n",
    "\n",
    "        if tracker_id in object_tags:\n",
    "            vehicle_data.append({\n",
    "                \"Frame\": index,\n",
    "                \"Center_X\": (bbox[0] + bbox[2]) / 2,\n",
    "                \"Center_Y\": (bbox[1] + bbox[3]) / 2,\n",
    "                \"Buffered_X1\": buffered_bbox[0],\n",
    "                \"Buffered_Y1\": buffered_bbox[1],\n",
    "                \"Buffered_X2\": buffered_bbox[2],\n",
    "                \"Buffered_Y2\": buffered_bbox[3],\n",
    "                \"Original_X1\": bbox[0],\n",
    "                \"Original_Y1\": bbox[1],\n",
    "                \"Original_X2\": bbox[2],\n",
    "                \"Original_Y2\": bbox[3],\n",
    "                \"Object ID\": tracker_id,\n",
    "                # \"Class ID\": class_id,\n",
    "                \"Line Tag\": object_tags[tracker_id]\n",
    "            })\n",
    "            \n",
    "    # Get the center coordinates of detected objects and apply perspective transformation\n",
    "    points = final_detections.get_anchors_coordinates(anchor=sv.Position.CENTER)\n",
    "    points_transformed = warp_points(points, perspective_matrix).astype(int)  \n",
    "\n",
    "    # Speed Estimation\n",
    "    labels = []\n",
    "    for tracker_id, (x, y) in zip(final_detections.tracker_id, points_transformed):\n",
    "        coordinates[tracker_id].append((x, y))  \n",
    "\n",
    "        if len(coordinates[tracker_id]) < video_info.fps / 2:\n",
    "            speed_mph = 0  \n",
    "        else:\n",
    "            (x1, y1) = coordinates[tracker_id][0]  \n",
    "            (x2, y2) = coordinates[tracker_id][-1]  \n",
    "\n",
    "            # Compute Euclidean distance (total displacement in pixels)\n",
    "            distance_pixels = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "\n",
    "            # Convert pixel distance to real-world meters\n",
    "            distance_meters = distance_pixels * np.mean([width_ratio, height_ratio])  \n",
    "\n",
    "            # Compute speed (meters per second → mph)\n",
    "            time = len(coordinates[tracker_id]) / video_info.fps  \n",
    "            speed_m_per_s = distance_meters / time  \n",
    "            speed_mph = speed_m_per_s * 2.23694  \n",
    "\n",
    "        # Add label with tracking ID and speed\n",
    "        labels.append(f\"#{tracker_id} {int(speed_mph)} mph\")\n",
    "\n",
    "        # Store tracking data for output\n",
    "        trajectory_data.append({\n",
    "            \"Frame\": index,\n",
    "            \"Tracker ID\": tracker_id,\n",
    "            \"Center X\": x,\n",
    "            \"Center Y\": y,\n",
    "            \"Speed (mph)\": speed_mph,\n",
    "            \"Line Tag\": object_tags.get(tracker_id, \"No Tag\")\n",
    "        })\n",
    "\n",
    "    # Draw ROI polygon on the frame\n",
    "    cv2.polylines(frame, [initial_polygon.astype(np.int32)], isClosed=True, color=(0, 0, 255), thickness=2)\n",
    "\n",
    "    # Annotate the video frame with bounding boxes and speed labels\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = bounding_box_annotator.annotate(scene=annotated_frame, detections=final_detections)\n",
    "    annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=final_detections, labels=labels)\n",
    "\n",
    "    return annotated_frame  \n",
    "\n",
    "# Process the video and save output\n",
    "sv.process_video(source_path=SOURCE_VIDEO_PATH, target_path=TARGET_VIDEO_PATH, callback=callback)\n",
    "\n",
    "# Save speed data to an Excel file\n",
    "trajectory_df = pd.DataFrame(trajectory_data)\n",
    "trajectory_df.to_excel(EXCEL_OUTPUT_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fadb7ed-934a-4012-9e94-c32bb0539c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hard_braking(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Detects hard braking incidents from an Excel file and saves the results in a new Excel file,\n",
    "    including the associated Line Tag.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_excel(input_file)\n",
    "\n",
    "    hard_braking_events = []\n",
    "\n",
    "    # Get unique vehicle IDs\n",
    "    unique_vehicles = df[\"Tracker ID\"].unique()\n",
    "\n",
    "    for vehicle_id in unique_vehicles:\n",
    "        vehicle_data = df[df[\"Tracker ID\"] == vehicle_id].sort_values(by=\"Frame\").reset_index(drop=True)\n",
    "\n",
    "        for i in range(len(vehicle_data) - 19):  # Ensuring a full 20-frame window\n",
    "            # Define the sliding window\n",
    "            window = vehicle_data.iloc[i : i + 20]\n",
    "\n",
    "            # Get initial and final speed\n",
    "            initial_speed = window.iloc[0][\"Speed (mph)\"]\n",
    "            final_speed = window.iloc[-1][\"Speed (mph)\"]\n",
    "\n",
    "            # Get the associated Line Tag (choosing the most frequent tag in the window)\n",
    "            line_tag = window[\"Line Tag\"].mode().iloc[0] if not window[\"Line Tag\"].mode().empty else \"Unknown\"\n",
    "\n",
    "            # Compute percentage drop in speed\n",
    "            if initial_speed > 0:  \n",
    "                percentage_drop = ((initial_speed - final_speed) / initial_speed) * 100\n",
    "\n",
    "                if percentage_drop > 70 and initial_speed > 15:\n",
    "                    # Check the next 100 frames for speed fluctuations\n",
    "                    post_window_data = vehicle_data.iloc[i + 20 : i + 120] if i + 120 < len(vehicle_data) else vehicle_data.iloc[i + 20 :]\n",
    "                    if not post_window_data.empty:\n",
    "                        max_post_speed = post_window_data[\"Speed (mph)\"].max()\n",
    "\n",
    "                        if max_post_speed <= final_speed + 5:  # Allow slight fluctuation\n",
    "                            # Check speed trend in previous 20 frames\n",
    "                            pre_window_data = vehicle_data.iloc[max(i - 20, 0) : i]\n",
    "\n",
    "                            if not pre_window_data.empty:\n",
    "                                min_pre_speed = pre_window_data[\"Speed (mph)\"].min()\n",
    "                                max_pre_speed = pre_window_data[\"Speed (mph)\"].max()\n",
    "\n",
    "                                if min_pre_speed >= initial_speed - 5 and max_pre_speed <= initial_speed + 5:\n",
    "                                    # Log hard braking event\n",
    "                                    hard_braking_events.append({\n",
    "                                        \"Tracker ID\": vehicle_id,\n",
    "                                        \"Frame Start\": window.iloc[0][\"Frame\"],\n",
    "                                        \"Frame End\": window.iloc[-1][\"Frame\"],\n",
    "                                        \"Initial Speed (mph)\": initial_speed,\n",
    "                                        \"Final Speed (mph)\": final_speed,\n",
    "                                        \"Percentage Drop\": percentage_drop,\n",
    "                                        \"Max Speed in Next 100 Frames\": max_post_speed,\n",
    "                                        \"Min Speed in Previous 20 Frames\": min_pre_speed,\n",
    "                                        \"Max Speed in Previous 20 Frames\": max_pre_speed,\n",
    "                                        \"Line Tag\": line_tag  \n",
    "                                    })\n",
    "\n",
    "    # Convert results into a DataFrame\n",
    "    hard_braking_df = pd.DataFrame(hard_braking_events)\n",
    "\n",
    "    # Save results to an Excel file\n",
    "    hard_braking_df.to_excel(output_file, index=False)\n",
    "\n",
    "    print(f\"Hard braking incidents saved to {output_file}\")\n",
    "\n",
    "detect_hard_braking(EXCEL_OUTPUT_PATH, FINAL_HARDBRAKING_DATA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ByteTrack)",
   "language": "python",
   "name": "bytetrack_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
