{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a46e96-0c6f-4915-8ee5-54a011ab2cde",
   "metadata": {},
   "source": [
    "**Vehicle Speed Estimation from Drone Footage**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43920e10-c44f-4b53-9c7c-dff863f49ea5",
   "metadata": {},
   "source": [
    "This script analyzes drone footage from a high-speed signalized intersection to detect and track vehicles, estimating their speeds using computer vision. It converts pixel movements into real-world speeds (mph) through perspective transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b549f0fc-a8f9-4208-a07e-e0877857f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q opencv-python numpy pandas tqdm ultralytics supervision xlsxwriter\n",
    "\n",
    "# Import essential libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import supervision as sv\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from supervision.assets import VideoAssets, download_assets\n",
    "from collections import defaultdict, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6143c96-b348-4e6d-a730-a22988285e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# File Paths: Define paths for input video, output video, and trajectory data storage\n",
    "EXCEL_OUTPUT_PATH = \"/Users/gideonowusu/Downloads/PhD 2028/Projects/UAV-TrafficVision/Trajectory Data.xlsx\"\n",
    "SOURCE_VIDEO_PATH = '/Users/gideonowusu/Downloads/PhD 2028/Projects/UAV-TrafficVision/Vehicle Speed Estimation from Drone Footage/High-Speed Intersection Video Sample.mp4'\n",
    "TARGET_VIDEO_PATH = '/Users/gideonowusu/Downloads/PhD 2028/Projects/UAV-TrafficVision/Vehicle Speed Estimation from Drone Footage/High-Speed Intersection Video Sample Output.mp4'\n",
    "\n",
    "# Real-World Measurements: Convert pixel values to meters using a known reference\n",
    "REAL_WORLD_WIDTH_M = 18  \n",
    "REAL_WORLD_HEIGHT_M = 120  \n",
    "PIXEL_WIDTH_PX = 25        \n",
    "PIXEL_HEIGHT_PX = 250      \n",
    "\n",
    "# Compute the scaling ratios to convert pixel measurements to meters\n",
    "width_ratio = REAL_WORLD_WIDTH_M / PIXEL_WIDTH_PX\n",
    "height_ratio = REAL_WORLD_HEIGHT_M / PIXEL_HEIGHT_PX\n",
    "\n",
    "# Perspective Transformation: Define the polygon for transformation (original → top-down view)\n",
    "initial_polygon = np.array([[270, 350], [460, 320], [1900, 660], [1720, 850]])  \n",
    "final_transformed = np.array([[0, 0], [24, 0], [24, 249], [0, 249]])  \n",
    "\n",
    "# Compute the transformation matrix to correct the perspective\n",
    "def get_perspective_transform(source, target):\n",
    "    \"\"\"Computes the transformation matrix from source to target perspective.\"\"\"\n",
    "    return cv2.getPerspectiveTransform(source.astype(np.float32), target.astype(np.float32))\n",
    "\n",
    "# Function to apply the transformation to points\n",
    "def warp_points(points, transform_matrix):\n",
    "    \"\"\"Applies a perspective warp transformation to a set of 2D points.\"\"\"\n",
    "    if not np.any(points):  \n",
    "        return points\n",
    "    float_points = np.expand_dims(points.astype(np.float32), axis=1)  \n",
    "    warped = cv2.perspectiveTransform(float_points, transform_matrix)  \n",
    "    return np.squeeze(warped, axis=1)  \n",
    "\n",
    "# Load customized YOLO Model for Drone Vehicle Detection (Use model of your choice)\n",
    "model = YOLO(\"/Users/gideonowusu/Downloads/PhD 2028/Projects/UAV-TrafficVision/Neuves.pt\")  \n",
    "\n",
    "# Get video metadata (frame rate, resolution, etc.)\n",
    "video_info = sv.VideoInfo.from_video_path(video_path=SOURCE_VIDEO_PATH)\n",
    "\n",
    "# Initialize ByteTrack for Object Tracking\n",
    "byte_track = sv.ByteTrack(\n",
    "    frame_rate=video_info.fps,  \n",
    "    track_activation_threshold=0.25,  \n",
    "    minimum_matching_threshold=0.8,  \n",
    "    lost_track_buffer=30  \n",
    ")\n",
    "\n",
    "# Annotators for Drawing on Video Frames\n",
    "bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=2)  \n",
    "label_annotator = sv.LabelAnnotator(text_scale=0.5, text_thickness=1, text_position=sv.Position.TOP_CENTER)  \n",
    "\n",
    "# Define a Region of Interest (ROI) for Tracking\n",
    "polygon_zone = sv.PolygonZone(polygon=initial_polygon)  \n",
    "\n",
    "# Dictionary to store object coordinates for speed estimation\n",
    "coordinates = defaultdict(lambda: deque(maxlen=video_info.fps))  \n",
    "trajectory_data = []  \n",
    "\n",
    "# Compute the transformation matrix once for efficiency\n",
    "perspective_matrix = get_perspective_transform(initial_polygon, final_transformed)\n",
    "\n",
    "# Frame Processing Function (Called for Each Video Frame)\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    \"\"\"Processes each video frame: detects vehicles, tracks them, and estimates speed.\"\"\"\n",
    "\n",
    "    global trajectory_data  \n",
    "\n",
    "    # Run YOLO detection on the frame\n",
    "    results = model(frame, verbose=False)[0]\n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "\n",
    "    # Separate detected utility poles (class_id = 1) from vehicles \n",
    "    pole_detections = detections[detections.class_id == 1]\n",
    "    non_pole_detections = detections[detections.class_id != 1]\n",
    "\n",
    "    # Only track vehicles inside the defined polygon (ignoring background detections)\n",
    "    non_pole_detections = non_pole_detections[polygon_zone.trigger(non_pole_detections)]\n",
    "    final_detections = byte_track.update_with_detections(detections=non_pole_detections)\n",
    "\n",
    "    # Get the center coordinates of detected objects and apply perspective transformation\n",
    "    points = final_detections.get_anchors_coordinates(anchor=sv.Position.CENTER)\n",
    "    points_transformed = warp_points(points, perspective_matrix).astype(int)  \n",
    "\n",
    "    # Speed Estimation\n",
    "    labels = []\n",
    "    for tracker_id, (x, y) in zip(final_detections.tracker_id, points_transformed):\n",
    "        coordinates[tracker_id].append((x, y))  \n",
    "\n",
    "        if len(coordinates[tracker_id]) < video_info.fps / 2:\n",
    "            speed_mph = 0  \n",
    "        else:\n",
    "            (x1, y1) = coordinates[tracker_id][0]  \n",
    "            (x2, y2) = coordinates[tracker_id][-1]  \n",
    "\n",
    "            # Compute Euclidean distance (total displacement in pixels)\n",
    "            distance_pixels = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "\n",
    "            # Convert pixel distance to real-world meters\n",
    "            distance_meters = distance_pixels * np.mean([width_ratio, height_ratio])  \n",
    "\n",
    "            # Compute speed (meters per second → mph)\n",
    "            time = len(coordinates[tracker_id]) / video_info.fps  \n",
    "            speed_m_per_s = distance_meters / time  \n",
    "            speed_mph = speed_m_per_s * 2.23694  \n",
    "\n",
    "        # Add label with tracking ID and speed\n",
    "        labels.append(f\"#{tracker_id} {int(speed_mph)} mph\")\n",
    "\n",
    "        # Store tracking data for output\n",
    "        trajectory_data.append({\n",
    "            \"Frame\": index,\n",
    "            \"Tracker ID\": tracker_id,\n",
    "            \"Center X\": x,\n",
    "            \"Center Y\": y,\n",
    "            \"Speed (mph)\": speed_mph\n",
    "        })\n",
    "\n",
    "    # Draw ROI polygon on the frame\n",
    "    cv2.polylines(frame, [initial_polygon.astype(np.int32)], isClosed=True, color=(0, 0, 255), thickness=2)\n",
    "\n",
    "    # Annotate the video frame with bounding boxes and speed labels\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = bounding_box_annotator.annotate(scene=annotated_frame, detections=final_detections)\n",
    "    annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=final_detections, labels=labels)\n",
    "\n",
    "    return annotated_frame  \n",
    "\n",
    "# Process the video and save output\n",
    "sv.process_video(source_path=SOURCE_VIDEO_PATH, target_path=TARGET_VIDEO_PATH, callback=callback)\n",
    "\n",
    "# Save speed data to an Excel file\n",
    "trajectory_df = pd.DataFrame(trajectory_data)\n",
    "trajectory_df.to_excel(EXCEL_OUTPUT_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11099c49-145f-4ce0-bcf6-e6e5419f0544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ByteTrack)",
   "language": "python",
   "name": "bytetrack_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
